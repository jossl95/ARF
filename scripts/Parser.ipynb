{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser\n",
    "\n",
    "This script parses files that are unpacked using `unpacker.ipynb`. \n",
    "\n",
    "## Reading in folder structure\n",
    "1. it defines a function(`get_directory_structure()`) to read in the folder structure as nested dictionaries\n",
    "2. it reads the folder structure and creates a nested dictionary. \n",
    "3. it writes the nested dictionary to as a JSON-dump\n",
    "\n",
    "\n",
    "## Parsing silkroad2\n",
    "*vendor information*\n",
    "- defines a parser to extract information about the vendor\n",
    "- interates over the folder structure and applies the ` parse_vendor_info()` to all vendor files\n",
    "- concatinates data to a datafile\n",
    "\n",
    "*feedback information*\n",
    "- defines a parser to extract feedbacks from seller pages. \n",
    "- iterates over the folder structure and applies the `parse_vendor_feedbacks()` to all vendor files\n",
    "- concatinates feedback-data to a datafile\n",
    "\n",
    "*item information*\n",
    "- defines a parser to extract feedbacks from item pages\n",
    "- iterates over the folder structure and applies the parser to all item files\n",
    "- iterates over the folder stucutre and apllies the `parse_item_information()` to all item files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from functools import reduce\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAINDIR = os.getcwd().rsplit('/', 1)[0]\n",
    "files = [folder for folder in os.listdir(os.path.join(MAINDIR, \"data\", \"unpacked\")) if \".DS_\" not in folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_structure(rootdir):\n",
    "    \"\"\"\n",
    "    Creates a nested dictionary that represents the folder structure of rootdir\n",
    "    \"\"\"\n",
    "\n",
    "    dir = {}\n",
    "    rootdir = rootdir.rstrip(os.sep)\n",
    "    start = rootdir.rfind(os.sep) + 1\n",
    "    for path, dirs, files in os.walk(rootdir):\n",
    "        folders = path[start:].split(os.sep)\n",
    "        subdir = dict.fromkeys(files)\n",
    "        parent = reduce(dict.get, folders[:-1], dir)\n",
    "        parent[folders[-1]] = subdir\n",
    "    return dir\n",
    "\n",
    "\n",
    "def clean(dirs):\n",
    "    return [d for d in dirs if \".DS_\" not in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_structure = get_directory_structure(os.path.join(DATA_DIR))\n",
    "\n",
    "# file = os.path.join(MAIN_DIR, 'data', 'logs', 'folder_structure.json')\n",
    "# with open(file, \"w\") as f:\n",
    "#     json.dump(folder_structure, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time efficient loading of folder structure\n",
    "if os.path.exists(os.path.join(MAINDIR, 'data', 'logs')):\n",
    "    os.makedir(os.path.join(MAINDIR, 'data', 'logs'))\n",
    "\n",
    "is os.path.exists(os.path.join('data', 'parsed')):\n",
    "    os.makedir(os.path.join(MAINDIR, 'data', 'parsed'))\n",
    "\n",
    "file = os.path.join(MAINDIR, 'data/logs', 'folder_structure.json')\n",
    "with open(file) as json_file:\n",
    "    folder_structure = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vendor Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_info(vendor):\n",
    "    \"\"\"\n",
    "    creates a line that contains: \n",
    "        name     = name of vendor\n",
    "        stime    = time of scrape\n",
    "        stime_dt = date of scrape\n",
    "        score    = vendor rating average (out of 100)\n",
    "        ctime    = time from creation\n",
    "        otime    = time last online\n",
    "        loc      = location\n",
    "        area     = area of operation\n",
    "\n",
    "    Args:\n",
    "        string of file path\n",
    "\n",
    "    Returns: \n",
    "        list of data\n",
    "    \"\"\"\n",
    "\n",
    "    # get information from file\n",
    "    data = [vendor.split(os.sep)[-1].split('.')[0],\n",
    "            os.stat(vendor).st_birthtime,\n",
    "            file.split(os.sep)[-3]]\n",
    "\n",
    "    # opens and parses the html file\n",
    "    with open(vendor, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        containers = soup.find_all('span', attrs={'class': 'container'})\n",
    "\n",
    "        # create list with name of vendor and scape date\n",
    "        # note that not in all instances vendor scores were present\n",
    "        # therefore the data is parsed conditionally\n",
    "        if \"vendor score:\" in containers[0].text:\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\n",
    "                    \".*: (.*).*\\n.*\\n.*for (.*)\\n.*: (.*)\\n.*: (.*)\\n.*: (.*)\", containers[0].text)[0]]\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            # stores an empty string when vendor scores are missing\n",
    "            data.append('')\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\n",
    "                    \"for.(.*)\\n.*:.(.*)\\n.*:.(.*)\\n.*:.(.*)\", containers[0].text)[0]]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_feedbacks(page):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        name      = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "\n",
    "    Args:\n",
    "        string of file path\n",
    "\n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # parses html table to dataframe\n",
    "    df = pd.DataFrame(pd.read_html(page, flavor=\"lxml\")[0])\n",
    "\n",
    "    # parses feedbacks into a {#} of 5 string format.\n",
    "    # Note that some ratings were notated as stars, these\n",
    "    # stars are counted and parsed as a string number\n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(\n",
    "            lambda x: \"{0} of 5\".format(x.count(\"â˜…\")))\n",
    "\n",
    "    # adds name and stime columns\n",
    "    df.assign(\n",
    "        name=str(page.split(os.sep)[-1].split('.')[0].split('?')[0]),\n",
    "        stime=os.stat(page).st_birthtime)\n",
    "\n",
    "    # reorders columns in data frame\n",
    "    df = df[['name', 'stime', 'rating', 'feedback', 'item', 'freshness']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_feedbacks(file):\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1.  It iterates over the dictionary containing the folder structure\n",
    "    2.  It asserts whethter the node in the directory is a file\n",
    "    3.  It parses files using ('parse_vendor_feedbacks()')\n",
    "    4.  It appends data to a container list\n",
    "    5.  For each date it concatinates the dataframes in the container\n",
    "        after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    # empty list for data frames\n",
    "    container = []\n",
    "    \n",
    "    # storing data.\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each user\n",
    "                if f in [\"users\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        if isinstance(sub, str):\n",
    "                            try:\n",
    "                                file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                            except: \n",
    "                                pass\n",
    "\n",
    "                            if os.path.isfile(file):\n",
    "                                try:\n",
    "                                    container.append(parse_vendor_feedbacks(file))\n",
    "                                except:\n",
    "                                    pass\n",
    "    \n",
    "    # concatenate all dataframes in container\n",
    "    df = pd.concat(container)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = parse_feedbacks(file)\n",
    "df.to_pickle(os.path.join(MAINDIR, 'data/parsed/silkroad2', 'feedbacks.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_item_information(file):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        price     = the price of the item that has been bought\n",
    "        vendor    = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "        stime_dt  = the date at which the item page was scraped\n",
    "        loc       = the country of operation of the vendor\n",
    "        area      = the area to which the vendor ships\n",
    "\n",
    "    Args:\n",
    "        string of file path\n",
    "\n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # get item meta data\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        \n",
    "    price = soup.find('div', attrs={'class': 'price_big'}).text.strip()\n",
    "    vendor = soup.find('h3').text.split(\": \")[1]\n",
    "    stime = os.stat(file).st_birthtime\n",
    "    stime_dt = file.split(os.sep)[-3]\n",
    "    p = soup.find_all('p', limit=2)[1]\n",
    "    area = re.findall(\".*to: (.*)$\",     p.text.strip())[0]\n",
    "    loc = re.findall(\"from: (.*)\\\\n.*\", p.text.strip())[0]\n",
    "    category = soup.find('div', attrs = {'class':'categories'})\n",
    "    item = soup.find('h2').text\n",
    "    category = soup.find('div', attrs = {'class':'categories'})\n",
    "    category = str(category.find('a',href = True)).split('/')[2].strip()\n",
    "\n",
    "    # parses feedback information for item\n",
    "    df = pd.read_html(file)[1].drop_duplicates()\n",
    "\n",
    "    # concats meta data to feedback information\n",
    "    df = df.assign(item=item,\n",
    "                   price=price,\n",
    "                   vendor=vendor,\n",
    "                   stime=stime,\n",
    "                   stime_dt=stime_dt,\n",
    "                   loc=loc,\n",
    "                   area=area, \n",
    "                   category = category)\n",
    "\n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(\n",
    "            lambda x: \"{0} of 5\".format(x.count(\"â˜…\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set out-file path\n",
    "\n",
    "data_folder = os.path.join(MAINDIR, \"data/parsed/silkroad2/items\")\n",
    "\n",
    "def file_name(data_folder, date):\n",
    "    return os.path.join(data_folder, ''.join(['items_', date.replace('-', ''), '.pickle']))\n",
    "\n",
    "# iterates over complex folder structure\n",
    "def parse_items():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1. It iterates over the dictionary containing the folder structure\n",
    "    2. It asserts whethter the node in the directory is a file\n",
    "    3. It parses files using ('parse_item_information()')\n",
    "    4. It appends data to a container list\n",
    "    5. For each date it concatinates the dataframes in the container\n",
    "       after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "      None\n",
    "    \n",
    "    Returns:\n",
    "      None\n",
    "      \n",
    "    Raises:\n",
    "      Contains a simply try clause that ensure that the parses will \n",
    "      continue in all cases. This circumvcirents issues caused by corrupted\n",
    "      files and encoding. \n",
    "    \"\"\"\n",
    "    \n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            # empty list for data frames\n",
    "            container = []\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each item\n",
    "                if f in [\"items\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        if os.path.isfile(file):\n",
    "                            try:\n",
    "                                container.append(parse_item_information(file))\n",
    "                            except:\n",
    "                                pass\n",
    "                        else: \n",
    "                            continue\n",
    "            \n",
    "            # append unique data to dfs and replenish the container\n",
    "            df = pd.concat(container, sort = True)\n",
    "            df = df.drop_duplicates()\n",
    "            df.to_pickle(file_name(data_folder, date))\n",
    "            \n",
    "            del container\n",
    "            del df\n",
    "\n",
    "parse_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = []\n",
    "\n",
    "for file in os.listdir(data_folder):\n",
    "    if '.pickle' in file:\n",
    "        df = pd.read_pickle(os.path.join(data_folder, file))\n",
    "        container.append(df)\n",
    "        \n",
    "df = pd.concat(container)\n",
    "df = df\\\n",
    "    .drop_duplicates()\\\n",
    "    .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = os.path.join(MAINDIR, 'data/parsed/silkroad2', 'items.pickle')\n",
    "%time\n",
    "df.to_pickle(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_category_information(file):\n",
    "    # operationalize container and column names\n",
    "    category = file.split(os.sep)[-2]\n",
    "    container = []\n",
    "    columns = ['title', 'vendor', 'location', 'area', 'price']\n",
    "\n",
    "    #read in html text\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'lxml')\n",
    "\n",
    "    # extract relevant data\n",
    "    items = soup.find_all('div', {'class': 'item_body'})\n",
    "    prices = soup.find_all('div', {'class': 'price_big'})\n",
    "\n",
    "    # parse data for listing\n",
    "    for item, price in zip(items, prices): \n",
    "        title = item.find('div', {'class': 'item_title'}).text\n",
    "        vendor, location, area = item.find('div', {'class': 'item_details'})\\\n",
    "            .text.strip()\\\n",
    "            .split('\\n      ')\n",
    "        price = price.text\n",
    "        container.append([title, vendor.split(': ')[-1], location.split(': ')[-1], area.split(': ')[-1], price])\n",
    "\n",
    "    df = pd.DataFrame.\\\n",
    "        from_records(container, columns = columns)\\\n",
    "        .drop_duplicates()\\\n",
    "        .reset_index(drop = True)\n",
    "    \n",
    "    df = df.assign(category = category)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_categories():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1.  It iterates over the dictionary containing the folder structure\n",
    "    2.  It asserts whethter the node in the directory is a file\n",
    "    3.  It parses files using ('parse_category_information()')\n",
    "    4.  It appends data to a container list\n",
    "    5.  For each date it concatinates the dataframes in the container\n",
    "        after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list for data frames\n",
    "    container = []\n",
    "    \n",
    "    # storing data.\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each user\n",
    "                if f in [\"categories\"]:\n",
    "                    for c in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        for sub in clean(folder_structure['unpacked'][market][date][f][c].keys()):\n",
    "                            if isinstance(sub, str):\n",
    "                                try:\n",
    "                                    file = os.path.join(DATA_DIR, market, date, f, c, sub)\n",
    "                                except: \n",
    "                                    pass\n",
    "\n",
    "                                if os.path.isfile(file):\n",
    "                                    try:\n",
    "                                        container.append(parse_category_information(file))\n",
    "                                    except:\n",
    "                                        pass\n",
    "    \n",
    "    # concatenate all dataframes in container\n",
    "    df = pd.concat(container)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = parse_categories()\n",
    "df.to_pickle(os.path.join(MAINDIR, 'data/parsed/silkroad2', 'categories.pickle'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}