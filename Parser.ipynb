{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "This script parses files that are unpacked using `unpacker.ipynb`. \n",
    "\n",
    "**Reading in folder structure**\n",
    "1. it defines a function(`get_directory_structure()`) to read in the folder structure as nested dictionaries\n",
    "2. it reads the folder structure and creates a nested dictionary. \n",
    "3. it writes the nested dictionary to as a JSON-dump\n",
    "\n",
    "\n",
    "**Parsing silkroad2**\n",
    "\n",
    "*vendor information*\n",
    "- defines a parser to extract information about the vendor\n",
    "- interates over the folder structure and applies the ` parse_vendor_info()` to all vendor files\n",
    "- concatinates data to a datafile\n",
    "\n",
    "*feedback information*\n",
    "- defines a parser to extract feedbacks from seller pages. \n",
    "- iterates over the folder structure and applies the `parse_vendor_feedbacks()` to all vendor files\n",
    "- concatinates feedback-data to a datafile\n",
    "\n",
    "*item information*\n",
    "- defines a parser to extract feedbacks from item pages\n",
    "- iterates over the folder structure and applies the parser to all item files\n",
    "- iterates over the folder stucutre and apllies the `parse_item_information()` to all item files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from functools import reduce\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"/Volumes/Extreme SSD\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\", \"unpacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Extreme SSD/data/unpacked'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [folder for folder in os.listdir(DATA_DIR) if \".DS_\" not in folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abraxas',\n",
       " 'agape',\n",
       " 'agora',\n",
       " 'alphabay',\n",
       " 'cloudnine',\n",
       " 'cryptomarket',\n",
       " 'diabolus',\n",
       " 'hydra',\n",
       " 'nucleus',\n",
       " 'outlawmarket',\n",
       " 'silkroad2',\n",
       " 'themarketplace']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_structure(rootdir):\n",
    "    \"\"\"\n",
    "    Creates a nested dictionary that represents the folder structure of rootdir\n",
    "    \"\"\"\n",
    "\n",
    "    dir = {}\n",
    "    rootdir = rootdir.rstrip(os.sep)\n",
    "    start = rootdir.rfind(os.sep) + 1\n",
    "    for path, dirs, files in os.walk(rootdir):\n",
    "        folders = path[start:].split(os.sep)\n",
    "        subdir = dict.fromkeys(files)\n",
    "        parent = reduce(dict.get, folders[:-1], dir)\n",
    "        parent[folders[-1]] = subdir\n",
    "    return dir\n",
    "\n",
    "\n",
    "def clean(dirs):\n",
    "    return [d for d in dirs if \".DS_\" not in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_structure = get_directory_structure(os.path.join(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = os.path.join(MAIN_DIR, 'data', 'logs', 'folder_structure.json')\n",
    "# with open(file, \"w\") as f:\n",
    "#     json.dump(folder_structure, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time efficient loading of folder structure\n",
    "file = os.path.join(MAIN_DIR, 'data', 'logs', 'folder_structure.json')\n",
    "with open(file) as json_file:\n",
    "    folder_structure = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silkroad2 Market\n",
    "### vendor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_info(vendor):\n",
    "    \"\"\"\n",
    "    creates a line that contains: \n",
    "        name     = name of vendor\n",
    "        stime    = time of scrape\n",
    "        stime_dt = date of scrape\n",
    "        score    = vendor rating average (out of 100)\n",
    "        ctime    = time from creation\n",
    "        otime    = time last online\n",
    "        loc      = location\n",
    "        area     = area of operation\n",
    "\n",
    "    Args:\n",
    "        string of file path\n",
    "\n",
    "    Returns: \n",
    "        list of data\n",
    "    \"\"\"\n",
    "\n",
    "    # get information from file\n",
    "    data = [vendor.split(os.sep)[-1].split('.')[0],\n",
    "            os.stat(vendor).st_birthtime,\n",
    "            file.split(os.sep)[-3]]\n",
    "\n",
    "    # opens and parses the html file\n",
    "    with open(vendor, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        containers = soup.find_all('span', attrs={'class': 'container'})\n",
    "\n",
    "        # create list with name of vendor and scape date\n",
    "        # note that not in all instances vendor scores were present\n",
    "        # therefore the data is parsed conditionally\n",
    "        if \"vendor score:\" in containers[0].text:\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\n",
    "                    \".*: (.*).*\\n.*\\n.*for (.*)\\n.*: (.*)\\n.*: (.*)\\n.*: (.*)\", containers[0].text)[0]]\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            # stores an empty string when vendor scores are missing\n",
    "            data.append('')\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\n",
    "                    \"for.(.*)\\n.*:.(.*)\\n.*:.(.*)\\n.*:.(.*)\", containers[0].text)[0]]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "problem with chuck10\n",
      "2014-08-30\n",
      "problem with chuck10\n",
      "2014-09-02\n",
      "problem with chuck10\n",
      "2014-09-10\n",
      "problem with chuck10\n",
      "2014-09-15\n",
      "problem with chuck10\n",
      "2014-09-20\n",
      "problem with chuck10\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "problem with chuckie11.1\n",
      "problem with chuckie11.2\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "def parse_vendors():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1. It iterates over the dictionary containing the folder structure\n",
    "    2. It asserts whethter the node in the directory is a file\n",
    "    3. It parses files using ('parse_item_information()')\n",
    "    4. It appends data to a container list\n",
    "    5. It creates a dataframe from the container (list of lists) \n",
    "       after which duplicates are dropped and columns are restructured\n",
    "    \"\"\"\n",
    "\n",
    "    # empty container for dataframes\n",
    "    container = []\n",
    "\n",
    "    # Iterate over folder structure\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            print(date)\n",
    "\n",
    "            # for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each category\n",
    "                if f in [\"users\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        if os.path.isfile(file):\n",
    "                            if \"?\" not in sub:\n",
    "                                try:\n",
    "                                    container.append(parse_vendor_info(file))\n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "    # construct data file from container\n",
    "    df = pd.DataFrame.from_records(container).drop_duplicates()\n",
    "    df.columns = ['name', 'stime', 'stime_dt',\n",
    "                  'score', 'ctime', 'otime', 'location', 'area']\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# storing data\n",
    "df = parse_vendor()\n",
    "df.to_pickle(os.path.join(MAIN_DIR, 'data', 'parsed',\n",
    "                          'silkroad2', 'vendors.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_feedbacks(page):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        name      = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "\n",
    "    Args:\n",
    "        string of file path\n",
    "\n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # parses html table to dataframe\n",
    "    df = pd.DataFrame(pd.read_html(page, flavor=\"lxml\")[0])\n",
    "\n",
    "    # parses feedbacks into a {#} of 5 string format.\n",
    "    # Note that some ratings were notated as stars, these\n",
    "    # stars are counted and parsed as a string number\n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(\n",
    "            lambda x: \"{0} of 5\".format(x.count(\"★\")))\n",
    "\n",
    "    # adds name and stime columns\n",
    "    df.assign(\n",
    "        name=str(page.split(os.sep)[-1].split('.')[0].split('?')[0]),\n",
    "        stime=os.stat(page).st_birthtime)\n",
    "\n",
    "    # reorders columns in data frame\n",
    "    df = df[['name', 'stime', 'rating', 'feedback', 'item', 'freshness']]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "2014-08-30\n",
      "2014-09-02\n",
      "2014-09-10\n",
      "2014-09-15\n",
      "2014-09-20\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "def parse_feedbacks()\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1.  It iterates over the dictionary containing the folder structure\n",
    "    2.  It asserts whethter the node in the directory is a file\n",
    "    3.  It parses files using ('parse_vendor_feedbacks()')\n",
    "    4.  It appends data to a container list\n",
    "    5.  For each date it concatinates the dataframes in the container\n",
    "        after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    # empty list for data frames\n",
    "    container = []\n",
    "    \n",
    "    # storing data.\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each user\n",
    "                if f in [\"users\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        if isinstance(sub, str):\n",
    "                            try:\n",
    "                                file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                            except: \n",
    "                                pass\n",
    "\n",
    "                            if os.path.isfile(file):\n",
    "                                try:\n",
    "                                    container.append(parse_vendor_feedbacks(file))\n",
    "                                except:\n",
    "                                    pass\n",
    "    \n",
    "    # concatenate all dataframes in container\n",
    "    df = pd.concat(container)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = parse_feedbacks()\n",
    "df.to_pickle(os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'feedbacks.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Volumes/Extreme SSD/data/unpacked/silkroad2/2014-05-24/items/0-1g-mdpv-90-pure'\n",
    "\n",
    "def parse_item_information(file):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        price     = the price of the item that has been bought\n",
    "        vendor    = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "        stime_dt  = the date at which the item page was scraped\n",
    "        loc       = the country of operation of the vendor\n",
    "        area      = the area to which the vendor ships\n",
    "\n",
    "    Args:\n",
    "        string of file path\n",
    "\n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # get item meta data\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        \n",
    "    price = soup.find('div', attrs={'class': 'price_big'}).text.strip()\n",
    "    vendor = soup.find('h3').text.split(\": \")[1]\n",
    "    stime = os.stat(file).st_birthtime\n",
    "    stime_dt = file.split(os.sep)[-3]\n",
    "    p = soup.find_all('p', limit=2)[1]\n",
    "    area = re.findall(\".*to: (.*)$\",     p.text.strip())[0]\n",
    "    loc = re.findall(\"from: (.*)\\\\n.*\", p.text.strip())[0]\n",
    "    category = soup.find('div', attrs = {'class':'categories'})\n",
    "    item = soup.find('h2').text\n",
    "    category = soup.find('div', attrs = {'class':'categories'})\n",
    "    category = str(category.find('a',href=True)).split('/')[2].strip()\n",
    "\n",
    "    # parses feedback information for item\n",
    "    df = pd.read_html(file)[1].drop_duplicates()\n",
    "\n",
    "    # concats meta data to feedback information\n",
    "    df = df.assign(item=item,\n",
    "                   price=price,\n",
    "                   vendor=vendor,\n",
    "                   stime=stime,\n",
    "                   stime_dt=stime_dt,\n",
    "                   loc=loc,\n",
    "                   area=area, \n",
    "                   category = category)\n",
    "\n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(\n",
    "            lambda x: \"{0} of 5\".format(x.count(\"★\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>feedback</th>\n",
       "      <th>freshness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 of 5</td>\n",
       "      <td>5/5 top notch</td>\n",
       "      <td>20 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5 of 5</td>\n",
       "      <td>Does the trick , quick delivery , not quite as...</td>\n",
       "      <td>65 days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                           feedback freshness\n",
       "0  5 of 5                                      5/5 top notch   20 days\n",
       "1  5 of 5  Does the trick , quick delivery , not quite as...   65 days"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_html(file)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "2014-08-30\n",
      "2014-09-02\n",
      "2014-09-10\n",
      "2014-09-15\n",
      "2014-09-20\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "# set out-file path\n",
    "data_folder = \"/Volumes/Extreme SSD/data/parsed/silkroad2/items\"\n",
    "\n",
    "def file_name(data_folder, date):\n",
    "    return os.path.join(data_folder, ''.join(['items_', date.replace('-', ''), '.pickle']))\n",
    "\n",
    "# iterates over complex folder structure\n",
    "def parse_items():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1. It iterates over the dictionary containing the folder structure\n",
    "    2. It asserts whethter the node in the directory is a file\n",
    "    3. It parses files using ('parse_item_information()')\n",
    "    4. It appends data to a container list\n",
    "    5. For each date it concatinates the dataframes in the container\n",
    "       after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "      None\n",
    "    \n",
    "    Returns:\n",
    "      None\n",
    "      \n",
    "    Raises:\n",
    "      Contains a simply try clause that ensure that the parses will \n",
    "      continue in all cases. This circumvcirents issues caused by corrupted\n",
    "      files and encoding. \n",
    "    \"\"\"\n",
    "    \n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            # empty list for data frames\n",
    "            container = []\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each item\n",
    "                if f in [\"items\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        if os.path.isfile(file):\n",
    "                            try:\n",
    "                                container.append(parse_item_information(file))\n",
    "                            except:\n",
    "                                pass\n",
    "                        else: \n",
    "                            continue\n",
    "            \n",
    "            # append unique data to dfs and replenish the container\n",
    "            df = pd.concat(container, sort = True)\n",
    "            df = df.drop_duplicates()\n",
    "            df.to_pickle(file_name(data_folder, date))\n",
    "            \n",
    "            del container\n",
    "            del df\n",
    "\n",
    "parse_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = []\n",
    "\n",
    "for file in os.listdir(data_folder):\n",
    "    if '.pickle' in file:\n",
    "        df = pd.read_pickle(os.path.join(data_folder, file))\n",
    "        container.append(df)\n",
    "        \n",
    "df = pd.concat(container)\n",
    "df = df\\\n",
    "    .drop_duplicates()\\\n",
    "    .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 23.8 µs\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'items.pickle')\n",
    "%time\n",
    "df.to_pickle(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/Volumes/Extreme SSD/data/unpacked/silkroad2/2013-12-20/categories/drugs/items\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>vendor</th>\n",
       "      <th>location</th>\n",
       "      <th>area</th>\n",
       "      <th>price</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200ug Shiva LSD Blotter - 1</td>\n",
       "      <td>aKid</td>\n",
       "      <td>Undeclared</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.010686</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.10g Washed Fishscale Cocaine</td>\n",
       "      <td>bcpltd</td>\n",
       "      <td>United States</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.015277</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1G MDMA 87% Purity , QUALITY GUARANTEED</td>\n",
       "      <td>haizenberg</td>\n",
       "      <td>Czech Republic</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.059410</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1g - White Crystal Clear - 83% (Labtested)</td>\n",
       "      <td>TeamDenmark</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Australia</td>\n",
       "      <td>฿0.200016</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Power Plant  3gr</td>\n",
       "      <td>online-cannabis-king</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.059334</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1G Amnesia Haze (Sativa)</td>\n",
       "      <td>MarleysMainMan</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>฿0.025461</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1G of Speed Paste 80%+</td>\n",
       "      <td>fredthebaker</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.016159</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1 GR Pure Cocaine</td>\n",
       "      <td>frankmatthews</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.185847</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150ug Hoffman Teddy Bears LSD Blotter - 1</td>\n",
       "      <td>aKid</td>\n",
       "      <td>Undeclared</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.010454</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30mg Oxycodone \"Roxys\"- PHARMACY FRESH!</td>\n",
       "      <td>SupremeQualityKing</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>฿0.037343</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1G...PURE &amp; UNCUT....FISHSCALE COCAINE</td>\n",
       "      <td>JustSmuggledN!</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>฿0.152768</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200ug Shiva LSD Blotter - 10</td>\n",
       "      <td>aKid</td>\n",
       "      <td>Undeclared</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.106862</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200ug Shiva LSD Blotter - 5</td>\n",
       "      <td>aKid</td>\n",
       "      <td>Undeclared</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.058774</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dutch Coffeeshop Best Pollen 1g</td>\n",
       "      <td>sugarwand</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.019424</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Xanax 2MG - Quad Bar</td>\n",
       "      <td>domesticdoode</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>฿0.007808</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1GR Pure Flake Cocaine</td>\n",
       "      <td>lostheaven</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.174232</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5 Nintendo Allstars | 170mg | UK Vendor</td>\n",
       "      <td>technohippy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>฿0.063822</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Angelina's Daze - Straspberry Suckers 1x</td>\n",
       "      <td>Angelina</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States &amp; Canada</td>\n",
       "      <td>฿0.010337</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>##### 0.25g Real Peruvian Cocaine Uncut  +87% ...</td>\n",
       "      <td>GermanTrustSupply</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.065046</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sponsor a Shadow Person! MDPV pre-order! Unlim...</td>\n",
       "      <td>nawlins</td>\n",
       "      <td>United States</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.042435</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1G of PURE UNCUT PERUVIAN COCAINE</td>\n",
       "      <td>fredthebaker</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.161611</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Facebook XTC Cube   210/230mg  MDMA</td>\n",
       "      <td>imyourepusher</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Undeclared</td>\n",
       "      <td>฿0.013939</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1g | Ketamine | UK Vendor</td>\n",
       "      <td>technohippy</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>฿0.055497</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Green Partyflock PILL (UK Vendor) 200 mg Speci...</td>\n",
       "      <td>sugarwand</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Worldwide</td>\n",
       "      <td>฿0.018037</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[5x] Dilaudid / Hydromorphone 8mg triangles</td>\n",
       "      <td>pirate_ship_revenge</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "      <td>฿0.169742</td>\n",
       "      <td>drugs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title                vendor  \\\n",
       "0                         200ug Shiva LSD Blotter - 1                  aKid   \n",
       "1                       .10g Washed Fishscale Cocaine                bcpltd   \n",
       "2             1G MDMA 87% Purity , QUALITY GUARANTEED            haizenberg   \n",
       "3          1g - White Crystal Clear - 83% (Labtested)           TeamDenmark   \n",
       "4                                    Power Plant  3gr  online-cannabis-king   \n",
       "5                            1G Amnesia Haze (Sativa)        MarleysMainMan   \n",
       "6                              1G of Speed Paste 80%+          fredthebaker   \n",
       "7                                   1 GR Pure Cocaine         frankmatthews   \n",
       "8           150ug Hoffman Teddy Bears LSD Blotter - 1                  aKid   \n",
       "9             30mg Oxycodone \"Roxys\"- PHARMACY FRESH!    SupremeQualityKing   \n",
       "10             1G...PURE & UNCUT....FISHSCALE COCAINE        JustSmuggledN!   \n",
       "11                       200ug Shiva LSD Blotter - 10                  aKid   \n",
       "12                        200ug Shiva LSD Blotter - 5                  aKid   \n",
       "13                    Dutch Coffeeshop Best Pollen 1g             sugarwand   \n",
       "14                               Xanax 2MG - Quad Bar         domesticdoode   \n",
       "15                             1GR Pure Flake Cocaine            lostheaven   \n",
       "16            5 Nintendo Allstars | 170mg | UK Vendor           technohippy   \n",
       "17           Angelina's Daze - Straspberry Suckers 1x              Angelina   \n",
       "18  ##### 0.25g Real Peruvian Cocaine Uncut  +87% ...     GermanTrustSupply   \n",
       "19  Sponsor a Shadow Person! MDPV pre-order! Unlim...               nawlins   \n",
       "20                  1G of PURE UNCUT PERUVIAN COCAINE          fredthebaker   \n",
       "21            The Facebook XTC Cube   210/230mg  MDMA         imyourepusher   \n",
       "22                          1g | Ketamine | UK Vendor           technohippy   \n",
       "23  Green Partyflock PILL (UK Vendor) 200 mg Speci...             sugarwand   \n",
       "24        [5x] Dilaudid / Hydromorphone 8mg triangles   pirate_ship_revenge   \n",
       "\n",
       "          location                    area      price category  \n",
       "0       Undeclared               Worldwide  ฿0.010686    drugs  \n",
       "1    United States               Worldwide  ฿0.015277    drugs  \n",
       "2   Czech Republic               Worldwide  ฿0.059410    drugs  \n",
       "3        Australia               Australia  ฿0.200016    drugs  \n",
       "4          Germany               Worldwide  ฿0.059334    drugs  \n",
       "5    United States           United States  ฿0.025461    drugs  \n",
       "6          Germany               Worldwide  ฿0.016159    drugs  \n",
       "7      Netherlands               Worldwide  ฿0.185847    drugs  \n",
       "8       Undeclared               Worldwide  ฿0.010454    drugs  \n",
       "9    United States           United States  ฿0.037343    drugs  \n",
       "10   United States           United States  ฿0.152768    drugs  \n",
       "11      Undeclared               Worldwide  ฿0.106862    drugs  \n",
       "12      Undeclared               Worldwide  ฿0.058774    drugs  \n",
       "13  United Kingdom               Worldwide  ฿0.019424    drugs  \n",
       "14   United States           United States  ฿0.007808    drugs  \n",
       "15         Germany               Worldwide  ฿0.174232    drugs  \n",
       "16  United Kingdom          United Kingdom  ฿0.063822    drugs  \n",
       "17   United States  United States & Canada  ฿0.010337    drugs  \n",
       "18         Germany               Worldwide  ฿0.065046    drugs  \n",
       "19   United States               Worldwide  ฿0.042435    drugs  \n",
       "20         Germany               Worldwide  ฿0.161611    drugs  \n",
       "21     Netherlands              Undeclared  ฿0.013939    drugs  \n",
       "22  United Kingdom          United Kingdom  ฿0.055497    drugs  \n",
       "23  United Kingdom               Worldwide  ฿0.018037    drugs  \n",
       "24   United States           United States  ฿0.169742    drugs  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_category_information(file):\n",
    "    # operationalize container and column names\n",
    "    category = file.split(os.sep)[-2]\n",
    "    container = []\n",
    "    columns = ['title', 'vendor', 'location', 'area', 'price']\n",
    "\n",
    "    #read in html text\n",
    "    with open(file, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'lxml')\n",
    "\n",
    "    # extract relevant data\n",
    "    items = soup.find_all('div', {'class': 'item_body'})\n",
    "    prices = soup.find_all('div', {'class': 'price_big'})\n",
    "\n",
    "    # parse data for listing\n",
    "    for item, price in zip(items, prices): \n",
    "        title = item.find('div', {'class': 'item_title'}).text\n",
    "        vendor, location, area = item.find('div', {'class': 'item_details'})\\\n",
    "            .text.strip()\\\n",
    "            .split('\\n      ')\n",
    "        price = price.text\n",
    "        container.append([title, vendor.split(': ')[-1], location.split(': ')[-1], area.split(': ')[-1], price])\n",
    "\n",
    "    df = pd.DataFrame.\\\n",
    "        from_records(container, columns = columns)\\\n",
    "        .drop_duplicates()\\\n",
    "        .reset_index(drop = True)\n",
    "    \n",
    "    df = df.assign(category = category)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "parse_category_information(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "2014-08-30\n",
      "2014-09-02\n",
      "2014-09-10\n",
      "2014-09-15\n",
      "2014-09-20\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "def parse_categories():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1.  It iterates over the dictionary containing the folder structure\n",
    "    2.  It asserts whethter the node in the directory is a file\n",
    "    3.  It parses files using ('parse_category_information()')\n",
    "    4.  It appends data to a container list\n",
    "    5.  For each date it concatinates the dataframes in the container\n",
    "        after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty list for data frames\n",
    "    container = []\n",
    "    \n",
    "    # storing data.\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each user\n",
    "                if f in [\"categories\"]:\n",
    "                    for c in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        for sub in clean(folder_structure['unpacked'][market][date][f][c].keys()):\n",
    "                            if isinstance(sub, str):\n",
    "                                try:\n",
    "                                    file = os.path.join(DATA_DIR, market, date, f, c, sub)\n",
    "                                except: \n",
    "                                    pass\n",
    "\n",
    "                                if os.path.isfile(file):\n",
    "                                    try:\n",
    "                                        container.append(parse_category_information(file))\n",
    "                                    except:\n",
    "                                        pass\n",
    "    \n",
    "    # concatenate all dataframes in container\n",
    "    df = pd.concat(container)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = parse_categories()\n",
    "df.to_pickle(os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'categories.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
