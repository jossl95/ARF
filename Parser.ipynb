{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "This script parses files that are unpacked using `unpacker.ipynb`. \n",
    "\n",
    "**Reading in folder structure**\n",
    "1. it defines a function(`get_directory_structure()`) to read in the folder structure as nested dictionaries\n",
    "2. it reads the folder structure and creates a nested dictionary. \n",
    "3. it writes the nested dictionary to as a JSON-dump\n",
    "\n",
    "\n",
    "**Parsing silkroad2**\n",
    "\n",
    "*vendor information*\n",
    "- defines a parser to extract information about the vendor\n",
    "- interates over the folder structure and applies the ` parse_vendor_info()` to all vendor files\n",
    "- concatinates data to a datafile\n",
    "\n",
    "*feedback information*\n",
    "- defines a parser to extract feedbacks from seller pages. \n",
    "- iterates over the folder structure and applies the `parse_vendor_feedbacks()` to all vendor files\n",
    "- concatinates feedback-data to a datafile\n",
    "\n",
    "*item information*\n",
    "- defines a parser to extract feedbacks from item pages\n",
    "- iterates over the folder structure and applies the parser to all item files\n",
    "- iterates over the folder stucutre and apllies the `parse_item_information()` to all item files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from functools import reduce\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"/Volumes/Extreme SSD\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\", \"unpacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Extreme SSD/data/unpacked'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [folder for folder in os.listdir(DATA_DIR) if \".DS_\" not in folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abraxas',\n",
       " 'agape',\n",
       " 'agora',\n",
       " 'alphabay',\n",
       " 'cloudnine',\n",
       " 'cryptomarket',\n",
       " 'diabolus',\n",
       " 'hydra',\n",
       " 'nucleus',\n",
       " 'outlawmarket',\n",
       " 'silkroad2',\n",
       " 'themarketplace']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_structure(rootdir):\n",
    "    \"\"\"\n",
    "    Creates a nested dictionary that represents the folder structure of rootdir\n",
    "    \"\"\"\n",
    "    \n",
    "    dir = {}\n",
    "    rootdir = rootdir.rstrip(os.sep)\n",
    "    start = rootdir.rfind(os.sep) + 1\n",
    "    for path, dirs, files in os.walk(rootdir):\n",
    "            folders = path[start:].split(os.sep)\n",
    "            subdir = dict.fromkeys(files)\n",
    "            parent = reduce(dict.get, folders[:-1], dir)\n",
    "            parent[folders[-1]] = subdir\n",
    "    return dir\n",
    "\n",
    "def clean(dirs):\n",
    "    return [d for d in dirs if \".DS_\" not in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_structure = get_directory_structure(os.path.join(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(MAIN_DIR, 'data', 'logs', 'folder_structure.json')\n",
    "with open(file, \"w\") as f:\n",
    "    json.dump(folder_structure, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silkroad2 Market\n",
    "### vendor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_info(vendor):\n",
    "    \"\"\"\n",
    "    creates a line that contains: \n",
    "        name     = name of vendor\n",
    "        stime    = time of scrape\n",
    "        stime_dt = date of scrape\n",
    "        score    = vendor rating average (out of 100)\n",
    "        ctime    = time from creation\n",
    "        otime    = time last online\n",
    "        loc      = location\n",
    "        area     = area of operation\n",
    "    \"\"\"\n",
    "    \n",
    "    # get information from file\n",
    "    data = [vendor.split(os.sep)[-1].split('.')[0], \n",
    "            os.stat(vendor).st_birthtime,\n",
    "            file.split(os.sep)[-3]]\n",
    "\n",
    "    with open(vendor, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        containers = soup.find_all('span', attrs = {'class': 'container'})\n",
    "\n",
    "        # create list with name of vendor and scape date\n",
    "        if \"vendor score:\" in containers[0].text:\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\".*: (.*).*\\n.*\\n.*for (.*)\\n.*: (.*)\\n.*: (.*)\\n.*: (.*)\", containers[0].text)[0]]\n",
    "            except: \n",
    "                print('problem with {0}'.format(sub))\n",
    "        else:\n",
    "            data.append('')\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\"for.(.*)\\n.*:.(.*)\\n.*:.(.*)\\n.*:.(.*)\", containers[0].text)[0]]\n",
    "            except: \n",
    "                print('problem with {0}'.format(sub))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "problem with chuck10\n",
      "2014-08-30\n",
      "problem with chuck10\n",
      "2014-09-02\n",
      "problem with chuck10\n",
      "2014-09-10\n",
      "problem with chuck10\n",
      "2014-09-15\n",
      "problem with chuck10\n",
      "2014-09-20\n",
      "problem with chuck10\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "problem with chuckie11.1\n",
      "problem with chuckie11.2\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "# empty container for dataframes\n",
    "container = []\n",
    "\n",
    "for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "    # for each date\n",
    "    for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "        print(date)\n",
    "\n",
    "        #for each folder\n",
    "        for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "            # for each category\n",
    "            if f in [\"users\"]:\n",
    "                for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                    file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                    if os.path.isfile(file):\n",
    "                        if \"?\" not in sub:\n",
    "                            try:\n",
    "                                container.append(parse_vendor_info(file))\n",
    "                            except: \n",
    "                                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing data\n",
    "df = pd.DataFrame.from_records(container).drop_duplicates()\n",
    "df.columns =  ['name', 'stime', 'stime_dt', 'score', 'ctime', 'otime', 'location', 'area']\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_pickle(os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'vendors.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_feedbacks(page):\n",
    "    df = pd.DataFrame(pd.read_html(page, flavor = \"lxml\")[0])\n",
    "    \n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(lambda x : \"{0} of 5\".format(x.count(\"â˜…\")))\n",
    "\n",
    "    df['name'] = str(page.split(os.sep)[-1].split('.')[0].split('?')[0])\n",
    "    df['stime'] = os.stat(page).st_birthtime\n",
    "    \n",
    "    df = df[['name', 'stime', 'rating', 'feedback', 'item', 'freshness']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "2014-08-30\n",
      "2014-09-02\n",
      "2014-09-10\n",
      "2014-09-15\n",
      "2014-09-20\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "# empty list for data frames\n",
    "container = []\n",
    "\n",
    "# storing data.\n",
    "for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "    # for each date\n",
    "    for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "        print(date)\n",
    "\n",
    "        #for each folder\n",
    "        for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "            # for each category\n",
    "            if f in [\"users\"]:\n",
    "                for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                    if isinstance(sub, str):\n",
    "                        try:\n",
    "                            file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        except: \n",
    "                            pass\n",
    "                            \n",
    "                        if os.path.isfile(file):\n",
    "                            try:\n",
    "                                container.append(parse_vendor_feedbacks(file))\n",
    "                            except:\n",
    "                                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 0 ns, total: 4 Âµs\n",
      "Wall time: 9.3 Âµs\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'feedbacks.pickle')\n",
    "%time\n",
    "df.to_pickle(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Volumes/Extreme SSD/data/unpacked/silkroad2/2014-05-24/items/0-1g-mdpv-90-pure'\n",
    "def parse_item_information(file):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        price     = the price of the item that has been bought\n",
    "        vendor    = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "        stime_dt  = the date at which the item page was scraped\n",
    "        loc       = the country of operation of the vendor\n",
    "        area      = the area to which the vendor ships\n",
    "    \"\"\"\n",
    "    \n",
    "    # get item meta data\n",
    "    with open(file, 'r') as f:\n",
    "        soup     = BeautifulSoup(f.read(), \"lxml\")\n",
    "        price    = soup.find('div', attrs = {'class': 'price_big'}).text.strip()\n",
    "        vendor   = soup.find('h3').text.split(\": \")[1]\n",
    "        stime    = os.stat(file).st_birthtime\n",
    "        stime_dt = file.split(os.sep)[-3]\n",
    "        p = soup.find_all('p', limit = 2)[1]\n",
    "        area     = re.findall(\".*to: (.*)$\",     p.text.strip())[0]\n",
    "        loc      = re.findall(\"from: (.*)\\\\n.*\", p.text.strip())[0]\n",
    "        item     = soup.find('h2').text\n",
    "\n",
    "    # parses feedback information for item\n",
    "    df = pd.read_html(file)[-1].drop_duplicates()\n",
    "\n",
    "    # concats meta data to feedback information\n",
    "    df = df.assign(item    = item,\n",
    "                  price    = price,\n",
    "                  vendor   = vendor, \n",
    "                  stime    = stime,\n",
    "                  stime_dt = stime_dt,\n",
    "                  loc      = loc,\n",
    "                  area     = area)\n",
    "    \n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(lambda x : \"{0} of 5\".format(x.count(\"â˜…\")))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n"
     ]
    }
   ],
   "source": [
    "# set out-file path\n",
    "data_folder = \"/Volumes/Extreme SSD/data/parsed/silkroad2/items\"\n",
    "\n",
    "def file_name(data_folder, date):\n",
    "    return os.path.join(data_folder, ''.join(['items_', date.replace('-', ''), '.pickle']))\n",
    "\n",
    "# iterates over complex folder structure\n",
    "def parse_items():\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            # empty list for data frames\n",
    "            container = []\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each category\n",
    "                if f in [\"items\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        if os.path.isfile(file):\n",
    "                            try:\n",
    "                                container.append(parse_item_information(file))\n",
    "                            except:\n",
    "                                pass\n",
    "                        else: \n",
    "                            continue\n",
    "            \n",
    "            # append unique data to dfs and replenish the container\n",
    "            df = pd.concat(container, sort = True)\n",
    "            df = df.drop_duplicates()\n",
    "            df.to_pickle(file_name(data_folder, date))\n",
    "            \n",
    "            \n",
    "            del container\n",
    "            del df\n",
    "\n",
    "parse_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Extreme SSD/data/parsed/silkroad2/items/items_20200502.pickle'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(container)\n",
    "data_file = os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'items.pickle')\n",
    "%time\n",
    "df.to_pickle(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
