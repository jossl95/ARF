{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "This script parses files that are unpacked using `unpacker.ipynb`. \n",
    "\n",
    "**Reading in folder structure**\n",
    "1. it defines a function(`get_directory_structure()`) to read in the folder structure as nested dictionaries\n",
    "2. it reads the folder structure and creates a nested dictionary. \n",
    "3. it writes the nested dictionary to as a JSON-dump\n",
    "\n",
    "\n",
    "**Parsing silkroad2**\n",
    "\n",
    "*vendor information*\n",
    "- defines a parser to extract information about the vendor\n",
    "- interates over the folder structure and applies the ` parse_vendor_info()` to all vendor files\n",
    "- concatinates data to a datafile\n",
    "\n",
    "*feedback information*\n",
    "- defines a parser to extract feedbacks from seller pages. \n",
    "- iterates over the folder structure and applies the `parse_vendor_feedbacks()` to all vendor files\n",
    "- concatinates feedback-data to a datafile\n",
    "\n",
    "*item information*\n",
    "- defines a parser to extract feedbacks from item pages\n",
    "- iterates over the folder structure and applies the parser to all item files\n",
    "- iterates over the folder stucutre and apllies the `parse_item_information()` to all item files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "from functools import reduce\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_DIR = \"/Volumes/Extreme SSD\"\n",
    "DATA_DIR = os.path.join(MAIN_DIR, \"data\", \"unpacked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Extreme SSD/data/unpacked'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [folder for folder in os.listdir(DATA_DIR) if \".DS_\" not in folder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abraxas',\n",
       " 'agape',\n",
       " 'agora',\n",
       " 'alphabay',\n",
       " 'cloudnine',\n",
       " 'cryptomarket',\n",
       " 'diabolus',\n",
       " 'hydra',\n",
       " 'nucleus',\n",
       " 'outlawmarket',\n",
       " 'silkroad2',\n",
       " 'themarketplace']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_structure(rootdir):\n",
    "    \"\"\"\n",
    "    Creates a nested dictionary that represents the folder structure of rootdir\n",
    "    \"\"\"\n",
    "    \n",
    "    dir = {}\n",
    "    rootdir = rootdir.rstrip(os.sep)\n",
    "    start = rootdir.rfind(os.sep) + 1\n",
    "    for path, dirs, files in os.walk(rootdir):\n",
    "            folders = path[start:].split(os.sep)\n",
    "            subdir = dict.fromkeys(files)\n",
    "            parent = reduce(dict.get, folders[:-1], dir)\n",
    "            parent[folders[-1]] = subdir\n",
    "    return dir\n",
    "\n",
    "def clean(dirs):\n",
    "    return [d for d in dirs if \".DS_\" not in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_structure = get_directory_structure(os.path.join(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(MAIN_DIR, 'data', 'logs', 'folder_structure.json')\n",
    "with open(file, \"w\") as f:\n",
    "    json.dump(folder_structure, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silkroad2 Market\n",
    "### vendor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_info(vendor):\n",
    "    \"\"\"\n",
    "    creates a line that contains: \n",
    "        name     = name of vendor\n",
    "        stime    = time of scrape\n",
    "        stime_dt = date of scrape\n",
    "        score    = vendor rating average (out of 100)\n",
    "        ctime    = time from creation\n",
    "        otime    = time last online\n",
    "        loc      = location\n",
    "        area     = area of operation\n",
    "    \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        list of data\n",
    "    \"\"\"\n",
    "    \n",
    "    # get information from file\n",
    "    data = [vendor.split(os.sep)[-1].split('.')[0], \n",
    "            os.stat(vendor).st_birthtime,\n",
    "            file.split(os.sep)[-3]]\n",
    "    \n",
    "    # opens and parses the html file\n",
    "    with open(vendor, 'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        containers = soup.find_all('span', attrs = {'class': 'container'})\n",
    "\n",
    "        # create list with name of vendor and scape date\n",
    "        # note that not in all instances vendor scores were present\n",
    "        # therefore the data is parsed conditionally\n",
    "        if \"vendor score:\" in containers[0].text:\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\".*: (.*).*\\n.*\\n.*for (.*)\\n.*: (.*)\\n.*: (.*)\\n.*: (.*)\", containers[0].text)[0]]\n",
    "            except: \n",
    "                pass\n",
    "        else:\n",
    "            # stores an empty string when vendor scores are missing\n",
    "            data.append('')\n",
    "            try:\n",
    "                [data.append(item) for item in re.findall(\"for.(.*)\\n.*:.(.*)\\n.*:.(.*)\\n.*:.(.*)\", containers[0].text)[0]]\n",
    "            except: \n",
    "                pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "problem with chuck10\n",
      "2014-08-30\n",
      "problem with chuck10\n",
      "2014-09-02\n",
      "problem with chuck10\n",
      "2014-09-10\n",
      "problem with chuck10\n",
      "2014-09-15\n",
      "problem with chuck10\n",
      "2014-09-20\n",
      "problem with chuck10\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "problem with chuckie11.1\n",
      "problem with chuckie11.2\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "def parse_vendors():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1. It iterates over the dictionary containing the folder structure\n",
    "    2. It asserts whethter the node in the directory is a file\n",
    "    3. It parses files using ('parse_item_information()')\n",
    "    4. It appends data to a container list\n",
    "    5. It creates a dataframe from the container (list of lists) \n",
    "       after which duplicates are dropped and columns are restructured\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty container for dataframes\n",
    "    container = []\n",
    "    \n",
    "    # Iterate over folder structure\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each category\n",
    "                if f in [\"users\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        if os.path.isfile(file):\n",
    "                            if \"?\" not in sub:\n",
    "                                try:\n",
    "                                    container.append(parse_vendor_info(file))\n",
    "                                except: \n",
    "                                    pass\n",
    "    \n",
    "    # construct data file from container\n",
    "    df = pd.DataFrame.from_records(container).drop_duplicates()\n",
    "    df.columns = ['name', 'stime', 'stime_dt', 'score', 'ctime', 'otime', 'location', 'area']\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# storing data\n",
    "df = parse_vendor()\n",
    "df.to_pickle(os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'vendors.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vendor_feedbacks(page):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        name      = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "        \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # parses html table to dataframe\n",
    "    df = pd.DataFrame(pd.read_html(page, flavor = \"lxml\")[0])\n",
    "    \n",
    "    # parses feedbacks into a {#} of 5 string format. \n",
    "    # Note that some ratings were notated as stars, these\n",
    "    # stars are counted and parsed as a string number\n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(lambda x : \"{0} of 5\".format(x.count(\"★\")))\n",
    "    \n",
    "    # adds name and stime columns \n",
    "    df.assign(\n",
    "        name  = str(page.split(os.sep)[-1].split('.')[0].split('?')[0]),\n",
    "        stime = os.stat(page).st_birthtime)\n",
    "    \n",
    "    #reorders columns in data frame\n",
    "    df = df[['name', 'stime', 'rating', 'feedback', 'item', 'freshness']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "2014-08-30\n",
      "2014-09-02\n",
      "2014-09-10\n",
      "2014-09-15\n",
      "2014-09-20\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "def parse_feedbacks()\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1.  It iterates over the dictionary containing the folder structure\n",
    "    2.  It asserts whethter the node in the directory is a file\n",
    "    3.  It parses files using ('parse_vendor_feedbacks()')\n",
    "    4.  It appends data to a container list\n",
    "    5.  For each date it concatinates the dataframes in the container\n",
    "        after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    # empty list for data frames\n",
    "    container = []\n",
    "    \n",
    "    # storing data.\n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each user\n",
    "                if f in [\"users\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        if isinstance(sub, str):\n",
    "                            try:\n",
    "                                file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                            except: \n",
    "                                pass\n",
    "\n",
    "                            if os.path.isfile(file):\n",
    "                                try:\n",
    "                                    container.append(parse_vendor_feedbacks(file))\n",
    "                                except:\n",
    "                                    pass\n",
    "    \n",
    "    # concatenate all dataframes in container\n",
    "    df = pd.concat(container)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = parse_feedbacks()\n",
    "df.to_pickle(os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'feedbacks.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/Volumes/Extreme SSD/data/unpacked/silkroad2/2014-05-24/items/0-1g-mdpv-90-pure'\n",
    "def parse_item_information(file):\n",
    "    \"\"\"\n",
    "    creates a dataframe that contains: \n",
    "        rating    = value containing the rating of the seller\n",
    "        feedback  = textual feedback \n",
    "        item      = item classifier\n",
    "        freshness = the freshness of the review\n",
    "        price     = the price of the item that has been bought\n",
    "        vendor    = the name of the vendor\n",
    "        stime     = the time at which the item-page was scraped\n",
    "        stime_dt  = the date at which the item page was scraped\n",
    "        loc       = the country of operation of the vendor\n",
    "        area      = the area to which the vendor ships\n",
    "        \n",
    "    Args:\n",
    "        string of file path\n",
    "    \n",
    "    Returns: \n",
    "        dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # get item meta data\n",
    "    with open(file, 'r') as f:\n",
    "        soup     = BeautifulSoup(f.read(), \"lxml\")\n",
    "        price    = soup.find('div', attrs = {'class': 'price_big'}).text.strip()\n",
    "        vendor   = soup.find('h3').text.split(\": \")[1]\n",
    "        stime    = os.stat(file).st_birthtime\n",
    "        stime_dt = file.split(os.sep)[-3]\n",
    "        p = soup.find_all('p', limit = 2)[1]\n",
    "        area     = re.findall(\".*to: (.*)$\",     p.text.strip())[0]\n",
    "        loc      = re.findall(\"from: (.*)\\\\n.*\", p.text.strip())[0]\n",
    "        item     = soup.find('h2').text\n",
    "\n",
    "    # parses feedback information for item\n",
    "    df = pd.read_html(file)[-1].drop_duplicates()\n",
    "\n",
    "    # concats meta data to feedback information\n",
    "    df = df.assign(item    = item,\n",
    "                  price    = price,\n",
    "                  vendor   = vendor, \n",
    "                  stime    = stime,\n",
    "                  stime_dt = stime_dt,\n",
    "                  loc      = loc,\n",
    "                  area     = area)\n",
    "    \n",
    "    if '5' not in df.rating[0]:\n",
    "        df['rating'] = df.rating.apply(lambda x : \"{0} of 5\".format(x.count(\"★\")))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-12-20\n",
      "2014-01-16\n",
      "2014-02-11\n",
      "2014-02-13\n",
      "2014-02-21\n",
      "2014-02-24\n",
      "2014-03-03\n",
      "2014-03-10\n",
      "2014-04-12\n",
      "2014-04-20\n",
      "2014-04-28\n",
      "2014-05-03\n",
      "2014-05-05\n",
      "2014-05-08\n",
      "2014-05-10\n",
      "2014-05-19\n",
      "2014-05-24\n",
      "2014-05-29\n",
      "2014-06-02\n",
      "2014-06-03\n",
      "2014-06-11\n",
      "2014-06-15\n",
      "2014-06-23\n",
      "2014-06-24\n",
      "2014-07-08\n",
      "2014-07-17\n",
      "2014-07-23\n",
      "2014-07-26\n",
      "2014-07-30\n",
      "2014-08-04\n",
      "2014-08-09\n",
      "2014-08-11\n",
      "2014-08-17\n",
      "2014-08-23\n",
      "2014-08-27\n",
      "2014-08-30\n",
      "2014-09-02\n",
      "2014-09-10\n",
      "2014-09-15\n",
      "2014-09-20\n",
      "2014-09-23\n",
      "2014-09-26\n",
      "2014-09-28\n",
      "2014-09-30\n",
      "2014-10-04\n",
      "2014-10-11\n",
      "2014-10-12\n",
      "2014-10-13\n",
      "2014-10-15\n",
      "2014-10-17\n",
      "2014-10-20\n",
      "2014-10-24\n",
      "2014-10-27\n",
      "2014-10-28\n",
      "2014-11-01\n",
      "2014-11-05\n",
      "2014-11-06\n"
     ]
    }
   ],
   "source": [
    "# set out-file path\n",
    "data_folder = \"/Volumes/Extreme SSD/data/parsed/silkroad2/items\"\n",
    "\n",
    "def file_name(data_folder, date):\n",
    "    return os.path.join(data_folder, ''.join(['items_', date.replace('-', ''), '.pickle']))\n",
    "\n",
    "# iterates over complex folder structure\n",
    "def parse_items():\n",
    "    \"\"\"\n",
    "    This function operates sequentially:\n",
    "    1. It iterates over the dictionary containing the folder structure\n",
    "    2. It asserts whethter the node in the directory is a file\n",
    "    3. It parses files using ('parse_item_information()')\n",
    "    4. It appends data to a container list\n",
    "    5. For each date it concatinates the dataframes in the container\n",
    "       after which the container is replenished to safe working memory\n",
    "    \n",
    "    Args:\n",
    "      None\n",
    "    \n",
    "    Returns:\n",
    "      None\n",
    "      \n",
    "    Raises:\n",
    "      Contains a simply try clause that ensure that the parses will \n",
    "      continue in all cases. This circumvcirents issues caused by corrupted\n",
    "      files and encoding. \n",
    "    \"\"\"\n",
    "    \n",
    "    for market in [i for i in clean(folder_structure['unpacked']) if \"silkroad2\" in i]:\n",
    "        # for each date\n",
    "        for date in clean(folder_structure['unpacked'][market].keys()):\n",
    "            # empty list for data frames\n",
    "            container = []\n",
    "            print(date)\n",
    "\n",
    "            #for each folder\n",
    "            for f in clean(folder_structure['unpacked'][market][date].keys()):\n",
    "\n",
    "                # for each item\n",
    "                if f in [\"items\"]:\n",
    "                    for sub in clean(folder_structure['unpacked'][market][date][f].keys()):\n",
    "                        file = os.path.join(DATA_DIR, market, date, f, sub)\n",
    "                        if os.path.isfile(file):\n",
    "                            try:\n",
    "                                container.append(parse_item_information(file))\n",
    "                            except:\n",
    "                                pass\n",
    "                        else: \n",
    "                            continue\n",
    "            \n",
    "            # append unique data to dfs and replenish the container\n",
    "            df = pd.concat(container, sort = True)\n",
    "            df = df.drop_duplicates()\n",
    "            df.to_pickle(file_name(data_folder, date))\n",
    "            \n",
    "            del container\n",
    "            del df\n",
    "\n",
    "parse_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = []\n",
    "\n",
    "for file in os.listdir(data_folder):\n",
    "    if '.pickle' in file:\n",
    "        df = pd.read_pickle(os.path.join(data_folder, file))\n",
    "        container.append(df)\n",
    "        \n",
    "df = pd.concat(container)\n",
    "df = df\\\n",
    "    .drop_duplicates()\\\n",
    "    .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'is_url' from 'pandas.io.common' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# need to escape the <class>, should be the first line.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"&lt;\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"&gt;\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"<pre>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"</pre>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_html\u001b[0;34m(self, classes, notebook, border)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 \u001b[0;31m# plus truncate dot col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m                 \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m                 \u001b[0;31m# '+ 1' to avoid too wide repr (GH PR #17023)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0madj_dif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdif\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/formats/html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from pandas.io.formats.format import (\n\u001b[1;32m     18\u001b[0m     \u001b[0mDataFrameFormatter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_url' from 'pandas.io.common' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/io/common.py)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          index       area                                           feedback  \\\n",
       "0             0  Worldwide                               didnt receive parcel   \n",
       "1             1  Worldwide  Good communication, next day delivery. Buds lo...   \n",
       "2             2  Worldwide  Never arrived. DO NOT FE WITH THIS SELLER or b...   \n",
       "3             3  Worldwide             great package and product will be back   \n",
       "4             4  Worldwide                                  Nothing arrived..   \n",
       "...         ...        ...                                                ...   \n",
       "22003613      5  Worldwide                                          thankyou!   \n",
       "22003614      6  Worldwide            quick delivery and product as described   \n",
       "22003615      7  Worldwide                                             Legend   \n",
       "22003616      8  Worldwide             Got it right away, go check it out....   \n",
       "22003617      9  Worldwide  I thought I would take a chance with these guy...   \n",
       "\n",
       "         freshness                                            item        loc  \\\n",
       "0           2 days  0.1g (One Point) of Pure, fluffy N, N-DMT ! :)  Australia   \n",
       "1           4 days  0.1g (One Point) of Pure, fluffy N, N-DMT ! :)  Australia   \n",
       "2           4 days  0.1g (One Point) of Pure, fluffy N, N-DMT ! :)  Australia   \n",
       "3           5 days  0.1g (One Point) of Pure, fluffy N, N-DMT ! :)  Australia   \n",
       "4           5 days  0.1g (One Point) of Pure, fluffy N, N-DMT ! :)  Australia   \n",
       "...            ...                                             ...        ...   \n",
       "22003613  115 days                       Zopiclone (Imovane) 7.5mg     Canada   \n",
       "22003614  115 days                       Zopiclone (Imovane) 7.5mg     Canada   \n",
       "22003615  118 days                       Zopiclone (Imovane) 7.5mg     Canada   \n",
       "22003616  119 days                       Zopiclone (Imovane) 7.5mg     Canada   \n",
       "22003617  121 days                       Zopiclone (Imovane) 7.5mg     Canada   \n",
       "\n",
       "              price  rating         stime    stime_dt           vendor  \n",
       "0         ฿0.055051  5 of 5  1.402429e+09  2013-12-20        sweetpuff  \n",
       "1         ฿0.055051  5 of 5  1.402429e+09  2013-12-20        sweetpuff  \n",
       "2         ฿0.055051  1 of 5  1.402429e+09  2013-12-20        sweetpuff  \n",
       "3         ฿0.055051  5 of 5  1.402429e+09  2013-12-20        sweetpuff  \n",
       "4         ฿0.055051  1 of 5  1.402429e+09  2013-12-20        sweetpuff  \n",
       "...             ...     ...           ...         ...              ...  \n",
       "22003613  ฿0.012044  5 of 5  1.415259e+09  2014-11-06  fierydice\\n54\\n  \n",
       "22003614  ฿0.012044  5 of 5  1.415259e+09  2014-11-06  fierydice\\n54\\n  \n",
       "22003615  ฿0.012044  5 of 5  1.415259e+09  2014-11-06  fierydice\\n54\\n  \n",
       "22003616  ฿0.012044  5 of 5  1.415259e+09  2014-11-06  fierydice\\n54\\n  \n",
       "22003617  ฿0.012044  5 of 5  1.415259e+09  2014-11-06  fierydice\\n54\\n  \n",
       "\n",
       "[22003618 rows x 11 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 2 µs, total: 10 µs\n",
      "Wall time: 538 µs\n"
     ]
    }
   ],
   "source": [
    "data_file = os.path.join(MAIN_DIR, 'data', 'parsed', 'silkroad2', 'items.pickle')\n",
    "%time\n",
    "df.to_pickle(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
